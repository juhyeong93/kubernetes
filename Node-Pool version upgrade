Cluster / node-pool Version upgrade시 고려사항 정리

PodDistruptionBudget이란.
항상 최소한의 Pod수를 유지하도록 도와주는 기능.

Check the pbd 

#kubectl get pdb -A
NAMESPACE      NAME                   MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
istio-system   istio-egressgateway    1               N/A               0                     306d
istio-system   istio-ingressgateway   1               N/A               0                     306d
istio-system   istiod                 1               N/A               0                     306d

MIN AVAILABLE 1 인상황에서 Cluster 및 node pool 업데이트를 진행하였더니 최소한의 파드가 1개는 무조건 유지가 되야하기때문에 업그레이드가 되지 않았음.
추가로 살펴봐야할것 

HPA ( Horizontal Pod Autoscaler)
hpa는 이름 그대로 pod를 수평적으로 스케일링 아웃함 /  replication controller, deployment, replicaset, statefulset 등의 워크로드 리소스를 scale out할 수 있다.
여기서 우리가 확인해야 할 부분은 replicaset 의 replicas의 개수 이다. 만약 replicas의 개수가 1개이고 PDB도 1개라면 controller는 1개만 유지하려고하고 PDB는 최소한 1개의 파드만 동작하려고
하는 상태에서 Node-pool version upgrade를 진행한다면 upgrade시 현재 동작하는 POD를 cordon(sheduling 금지)후 새로운 Pod를 생성해야하는데 replicas는 무조건 1개의 파드만 동작해야한다고
정의 되있고 PDB는 최소한 한개를 유지해야게끔 정의 되어있기때문에 Node-pool 업그레이드시 에러가 발생 했었다.

$ kubectl get hpa -A
NAMESPACE      NAME                   REFERENCE                         TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
istio-system   istio-ingressgateway   Deployment/istio-ingressgateway   4%/80%    1         5         1          365d
istio-system   istiod                 Deployment/istiod                 1%/80%    1         5         1          365d

kubectl edit deployment istio-ingressgateway -n istio-system
spec:
  replicas:1 
replicas 동작 확인 가능


kubectl edit hpa -n istio-system


이슈 해결 두개의 솔루션중 하나 선택해서 진행.


1. PDB를 0으로 바꾸고 업그레이드 진행
#kubectl edit pdb istio-ingressgateway -n istio-system           -> kubectl edit pdb [NAME] -n [NAMESPACE NAME]
MinAvailable: 1 -> 0으로 변경


2. replicas를 2로 변경후 진핸( 이럴시 PDB 값 1에 대한 충족하기때문에 이상없이 업그레이드 가능)

#kubectl get hpa -n istio-system
MinReplicas: 1 -> 2로 변경

K8S 구성환경을 잘 확인 해야하는데 region으로 cluster를 구성한 경우는 아래와 같이 수행
gcloud container clusters upgrade CLUSTER_NAME --node-pool=NODE_POOL_NAME --region=[REGION]

Test 서버나 많은 노드가 필요하지 않은 cluster들에 대해서 region이 아닌 zone 으로만 구성 했다면 아래와 같이 수행
gcloud container clusters upgrade CLUSTER_NAME --node-pool=NODE_POOL_NAME --zone=[ZONE]


**에러사항 추가
위와 같이 수행을 하면 문제없이 업그레이드가 잘 진행되지만 특정 프로젝트에서(node pool이 많이 동작하는 cluster) 업그레이드시 "[ip address in pool is exhausted]"
에러가 났다 이 에러는 VPC내 pod를 위해 할당한 subnet iprange범위에 넘어서서 에러가 발생.

해결방법 

1. subnet iprange를 늘려준다 초래
 - 기본적으로 ip range를 한번 늘리면 다시 범위를 줄일수 없기때문에 업그레이드만을 위해 ip를 늘려준다면 유휴ip들이 남아 있어 ip 낭비 초래
 
2. Default node zone을 1개로 줄이고 업그레이드 진행후 원복. (이 방법 선택하여 진행)
Region:	
                     asia-southeast1	
Default node zones: 	
                    asia-southeast1-b
                    asia-southeast1-a
                    asia-southeast1-c

현재 구성이 위와 같이 되있었다면 특정노드풀을 선책하고 Edit을 선택하여 asia-southeast1-b만 남기고 저장
각 zone의 노드에 있었던 모든 파드들이 하나로 줄어들면서 유휴 ip도 생겨 업그레이드 진행시 문제 없이 진행됨

보통 노드풀을 만들때 Maximum Pods per Node가 110 defualt로 설정 되는데 시스템 규모를 고려하여 cluster및 nodepool 생성시 옵션으로 pod 개수를 조정 필요.
--default-max-pods-per-node "32"


